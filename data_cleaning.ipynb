{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c63a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b019b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse PIRATE calibration filename and extract relevant information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        Original PIRATE filename\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Dictionary with parsed information or None if parsing fails\n",
    "    \"\"\"\n",
    "    # Remove .fits extension\n",
    "    name = filename.replace('.fits', '')\n",
    "    \n",
    "    # Split by underscores\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    # Determine if it's a flat or bias\n",
    "    if 'flats' in name.lower():\n",
    "        frame_type = 'flat'\n",
    "        # Format: PIRATE_NUMBER_flats_FILTER_ID_YYYY_MM_DD_HH_MM_SS\n",
    "        try:\n",
    "            filter_band = parts[3]  # B, V, R, etc.\n",
    "            frame_id = parts[4]\n",
    "            date_parts = parts[5:8]  # YYYY, MM, DD\n",
    "            date = '_'.join(date_parts)\n",
    "            \n",
    "            return {\n",
    "                'type': frame_type,\n",
    "                'filter': filter_band,\n",
    "                'date': date,\n",
    "                'id': frame_id,\n",
    "                'original': filename\n",
    "            }\n",
    "        except IndexError:\n",
    "            print(f\"Could not parse flat file: {filename}\")\n",
    "            return None\n",
    "            \n",
    "    elif 'bias' in name.lower():\n",
    "        frame_type = 'bias'\n",
    "        # Format: PIRATE_NUMBER_Bias22_ID_YYYY_MM_DD_HH_MM_SS\n",
    "        try:\n",
    "            frame_id = parts[3]\n",
    "            date_parts = parts[4:7]  # YYYY, MM, DD\n",
    "            date = '_'.join(date_parts)\n",
    "            \n",
    "            return {\n",
    "                'type': frame_type,\n",
    "                'filter': None,  # Bias frames don't have filters\n",
    "                'date': date,\n",
    "                'id': frame_id,\n",
    "                'original': filename\n",
    "            }\n",
    "        except IndexError:\n",
    "            print(f\"Could not parse bias file: {filename}\")\n",
    "            return None\n",
    "    \n",
    "    else:\n",
    "        print(f\"Unknown file type: {filename}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_new_filename(info):\n",
    "    \"\"\"\n",
    "    Create simplified filename from parsed information.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    info : dict\n",
    "        Parsed file information\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        New simplified filename\n",
    "    \"\"\"\n",
    "    if info['type'] == 'flat':\n",
    "        # Format: YYYY_MM_DD_FILTER_flat_ID.fits\n",
    "        new_name = f\"{info['date']}_{info['filter']}_flat_{info['id'].zfill(2)}.fits\"\n",
    "    else:  # bias\n",
    "        # Format: YYYY_MM_DD_bias_ID.fits\n",
    "        new_name = f\"{info['date']}_bias_{info['id'].zfill(2)}.fits\"\n",
    "    \n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7e030",
   "metadata": {},
   "source": [
    "## Test Parsing on Sample Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb838cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the parsing function\n",
    "test_files = [\n",
    "    'PIRATE_158371_flats_R_01_2025_09_01_19_54_25.fits',\n",
    "    'PIRATE_161217_Bias22_0_2025_09_20_18_36_49.fits',\n",
    "    'PIRATE_162424_flats_B_04_2025_09_25_19_17_25.fits',\n",
    "    'PIRATE_163073_Bias22_1_2025_09_29_18_25_45.fits'\n",
    "]\n",
    "\n",
    "print(\"Testing filename parsing:\")\n",
    "print(\"=\" * 70)\n",
    "for filename in test_files:\n",
    "    info = parse_filename(filename)\n",
    "    if info:\n",
    "        new_name = create_new_filename(info)\n",
    "        print(f\"Original: {filename}\")\n",
    "        print(f\"New:      {new_name}\")\n",
    "        print(f\"Type:     {info['type']}, Filter: {info['filter']}, Date: {info['date']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba10b72",
   "metadata": {},
   "source": [
    "## Preview Reorganization (Dry Run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "data_dir = Path('../data')\n",
    "calibration_dir = data_dir / 'calibration'\n",
    "\n",
    "print(f\"Calibration directory: {calibration_dir.absolute()}\")\n",
    "print()\n",
    "\n",
    "# Find all week directories\n",
    "week_dirs = sorted([d for d in calibration_dir.glob('week*') if d.is_dir()])\n",
    "\n",
    "if not week_dirs:\n",
    "    print(f\"No week directories found in {calibration_dir}\")\n",
    "else:\n",
    "    print(f\"Found {len(week_dirs)} week directories:\")\n",
    "    for week_dir in week_dirs:\n",
    "        fits_count = len(list(week_dir.glob('*.fits')))\n",
    "        print(f\"  {week_dir.name}: {fits_count} FITS files\")\n",
    "    print()\n",
    "    \n",
    "    # Preview first few files from each week\n",
    "    print(\"Sample files (first 2 from each week):\")\n",
    "    print(\"=\" * 70)\n",
    "    for week_dir in week_dirs:\n",
    "        print(f\"\\n{week_dir.name}:\")\n",
    "        fits_files = sorted(week_dir.glob('*.fits'))[:2]\n",
    "        for fits_file in fits_files:\n",
    "            info = parse_filename(fits_file.name)\n",
    "            if info:\n",
    "                new_name = create_new_filename(info)\n",
    "                print(f\"  {fits_file.name}\")\n",
    "                print(f\"  -> {info['type']}/{new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc52417a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorganize_calibration_files(calibration_dir, dry_run=True):\n",
    "    \"\"\"\n",
    "    Reorganize calibration files from week folders into flats/ and bias/ folders.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    calibration_dir : Path\n",
    "        Path to calibration directory containing week1, week2, etc.\n",
    "    dry_run : bool\n",
    "        If True, only print what would be done without making changes\n",
    "    \"\"\"\n",
    "    calibration_dir = Path(calibration_dir)\n",
    "    \n",
    "    # Create new directory structure\n",
    "    flats_dir = calibration_dir / 'flats'\n",
    "    bias_dir = calibration_dir / 'bias'\n",
    "    \n",
    "    if not dry_run:\n",
    "        flats_dir.mkdir(exist_ok=True)\n",
    "        bias_dir.mkdir(exist_ok=True)\n",
    "        print(f\"Created directories:\")\n",
    "        print(f\"   {flats_dir}\")\n",
    "        print(f\"   {bias_dir}\")\n",
    "    else:\n",
    "        print(f\"[DRY RUN] Would create directories:\")\n",
    "        print(f\"   {flats_dir}\")\n",
    "        print(f\"   {bias_dir}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Find all week directories\n",
    "    week_dirs = sorted([d for d in calibration_dir.glob('week*') if d.is_dir()])\n",
    "    \n",
    "    if not week_dirs:\n",
    "        print(f\"No week directories found in {calibration_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(week_dirs)} week directories: {[d.name for d in week_dirs]}\\n\")\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'flats_moved': 0,\n",
    "        'bias_moved': 0,\n",
    "        'errors': 0,\n",
    "        'duplicates': 0\n",
    "    }\n",
    "    \n",
    "    # Track new filenames to detect duplicates\n",
    "    new_filenames = {'flat': set(), 'bias': set()}\n",
    "    \n",
    "    # Process each week directory\n",
    "    for week_dir in week_dirs:\n",
    "        print(f\"Processing {week_dir.name}...\")\n",
    "        \n",
    "        # Find all FITS files\n",
    "        fits_files = sorted(week_dir.glob('*.fits'))\n",
    "        \n",
    "        if not fits_files:\n",
    "            print(f\"  No FITS files found in {week_dir.name}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Found {len(fits_files)} FITS files\")\n",
    "        \n",
    "        for fits_file in fits_files:\n",
    "            # Parse filename\n",
    "            info = parse_filename(fits_file.name)\n",
    "            \n",
    "            if info is None:\n",
    "                stats['errors'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Create new filename\n",
    "            new_filename = create_new_filename(info)\n",
    "            \n",
    "            # Determine destination directory\n",
    "            if info['type'] == 'flat':\n",
    "                dest_dir = flats_dir\n",
    "                file_type = 'flat'\n",
    "            else:\n",
    "                dest_dir = bias_dir\n",
    "                file_type = 'bias'\n",
    "            \n",
    "            dest_path = dest_dir / new_filename\n",
    "            \n",
    "            # Check for duplicates\n",
    "            if new_filename in new_filenames[file_type]:\n",
    "                print(f\" DUPLICATE: {new_filename}\")\n",
    "                # Add week number to filename to make it unique\n",
    "                name_parts = new_filename.rsplit('.', 1)\n",
    "                new_filename = f\"{name_parts[0]}_{week_dir.name}.{name_parts[1]}\"\n",
    "                dest_path = dest_dir / new_filename\n",
    "                stats['duplicates'] += 1\n",
    "            \n",
    "            new_filenames[file_type].add(new_filename)\n",
    "            \n",
    "            # Move or copy file\n",
    "            if dry_run:\n",
    "                if fits_files.index(fits_file) < 2:  # Only show first 2 per week\n",
    "                    print(f\"  [DRY RUN] {fits_file.name} -> {file_type}/{new_filename}\")\n",
    "            else:\n",
    "                try:\n",
    "                    shutil.copy2(fits_file, dest_path)\n",
    "                    if info['type'] == 'flat':\n",
    "                        stats['flats_moved'] += 1\n",
    "                    else:\n",
    "                        stats['bias_moved'] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\" Error copying {fits_file.name}: {e}\")\n",
    "                    stats['errors'] += 1\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    if dry_run:\n",
    "        print(\"[DRY RUN - No files were actually moved]\")\n",
    "    print(f\"Flat fields: {stats['flats_moved']}\")\n",
    "    print(f\"Bias frames: {stats['bias_moved']}\")\n",
    "    print(f\"Duplicates handled: {stats['duplicates']}\")\n",
    "    print(f\"Errors: {stats['errors']}\")\n",
    "    print()\n",
    "    \n",
    "    if not dry_run:\n",
    "        print(\"âœ… Reorganization complete!\")\n",
    "        print()\n",
    "        print(\"Original week directories are preserved.\")\n",
    "        print(\"You can delete them manually if reorganization looks correct.\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943a5ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run in DRY RUN mode first to preview\n",
    "print(\"DRY RUN - Preview of reorganization\")\n",
    "print(\"=\" * 70)\n",
    "stats = reorganize_calibration_files(calibration_dir, dry_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed77f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN THIS CELL TO ACTUALLY REORGANIZE FILES\n",
    "print(\"ACTUAL RUN - Reorganizing files\")\n",
    "print(\"=\" * 70)\n",
    "stats = reorganize_calibration_files(calibration_dir, dry_run=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85d08e",
   "metadata": {},
   "source": [
    "## Verify New Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the new directory structure\n",
    "flats_dir = calibration_dir / 'flats'\n",
    "bias_dir = calibration_dir / 'bias'\n",
    "\n",
    "if flats_dir.exists():\n",
    "    flats = sorted(flats_dir.glob('*.fits'))\n",
    "    print(f\"Flats directory: {len(flats)} files\")\n",
    "    print(\"Sample files:\")\n",
    "    for f in flats[:5]:\n",
    "        print(f\"  {f.name}\")\n",
    "    if len(flats) > 5:\n",
    "        print(f\"  ... and {len(flats) - 5} more\")\n",
    "else:\n",
    "    print(\"Flats directory not created yet\")\n",
    "\n",
    "print()\n",
    "\n",
    "if bias_dir.exists():\n",
    "    bias = sorted(bias_dir.glob('*.fits'))\n",
    "    print(f\"Bias directory: {len(bias)} files\")\n",
    "    print(\"Sample files:\")\n",
    "    for f in bias[:5]:\n",
    "        print(f\"  {f.name}\")\n",
    "    if len(bias) > 5:\n",
    "        print(f\"  ... and {len(bias) - 5} more\")\n",
    "else:\n",
    "    print(\"Bias directory not created yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27fc99be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: PIRATE_164643_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_14_11.fits -> PIRATE_164643_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_14_11.fits\n",
      "Renamed: PIRATE_164645_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_02_2025_10_06_20_16_17.fits -> PIRATE_164645_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_02_2025_10_06_20_16_17.fits\n",
      "Renamed: PIRATE_164635_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_00_2025_10_06_20_07_50.fits -> PIRATE_164635_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_00_2025_10_06_20_07_50.fits\n",
      "Renamed: PIRATE_164632_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_00_2025_10_06_20_06_45.fits -> PIRATE_164632_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_00_2025_10_06_20_06_45.fits\n",
      "Renamed: PIRATE_164637_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_00_2025_10_06_20_08_37.fits -> PIRATE_164637_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_00_2025_10_06_20_08_37.fits\n",
      "Renamed: PIRATE_164646_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_01_2025_10_06_20_16_39.fits -> PIRATE_164646_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_01_2025_10_06_20_16_39.fits\n",
      "Renamed: PIRATE_164652_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_22_51.fits -> PIRATE_164652_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_22_51.fits\n",
      "Renamed: PIRATE_164644_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_00_2025_10_06_20_15_55.fits -> PIRATE_164644_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_00_2025_10_06_20_15_55.fits\n",
      "Renamed: PIRATE_164633_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_01_2025_10_06_20_07_02.fits -> PIRATE_164633_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_01_2025_10_06_20_07_02.fits\n",
      "Renamed: PIRATE_164641_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_11_06.fits -> PIRATE_164641_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_11_06.fits\n",
      "Renamed: PIRATE_164636_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_01_2025_10_06_20_08_07.fits -> PIRATE_164636_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_01_2025_10_06_20_08_07.fits\n",
      "Renamed: PIRATE_164638_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_02_2025_10_06_20_09_19.fits -> PIRATE_164638_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_R_02_2025_10_06_20_09_19.fits\n",
      "Renamed: PIRATE_164650_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_21_46.fits -> PIRATE_164650_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_21_46.fits\n",
      "Renamed: PIRATE_164642_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_12_39.fits -> PIRATE_164642_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_12_39.fits\n",
      "Renamed: PIRATE_164649_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_19_17.fits -> PIRATE_164649_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_02_2025_10_06_20_19_17.fits\n",
      "Renamed: PIRATE_164648_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_18_15.fits -> PIRATE_164648_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_18_15.fits\n",
      "Renamed: PIRATE_164639_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_01_2025_10_06_20_09_49.fits -> PIRATE_164639_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_01_2025_10_06_20_09_49.fits\n",
      "Renamed: PIRATE_164647_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_17_13.fits -> PIRATE_164647_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_00_2025_10_06_20_17_13.fits\n",
      "Renamed: PIRATE_164651_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_22_19.fits -> PIRATE_164651_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_B_01_2025_10_06_20_22_19.fits\n",
      "Renamed: PIRATE_164634_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_02_2025_10_06_20_07_19.fits -> PIRATE_164634_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_I_02_2025_10_06_20_07_19.fits\n",
      "Renamed: PIRATE_164640_OLS_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_02_2025_10_06_20_10_21.fits -> PIRATE_164640_OSL_ROE_EXO1_SA110_506_star2_00_SA110_506_00_Filter_V_02_2025_10_06_20_10_21.fits\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the directory containing your files\n",
    "standards_dir = Path(\"data/raw/standard_stars\")\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for file in standards_dir.glob(\"*star2*.fits\"):\n",
    "    new_name = file.name.replace(\"OLS\", \"OSL\")\n",
    "    new_path = file.parent / new_name\n",
    "    os.rename(file, new_path)\n",
    "    print(f\"Renamed: {file.name} -> {new_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195305b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoplanets_TGP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
